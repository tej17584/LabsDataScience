library(cluster) #Para calcular la silueta
library(e1071)#para cmeans
library(mclust) #mixtures of gaussians
library(fpc) #para hacer el plotcluster
library(NbClust) #Para determinar el número de clusters óptimo
library(factoextra) #Para hacer gráficos bonitos de clustering

#k-medias
datos<-iris
irisCompleto<-iris[complete.cases(iris),]
km<-kmeans(iris[,1:4],3)
datos$grupo<-km$cluster

g1<- datos[datos$grupo==1,]
prop.table(table(g1$Species))*100
nrow(g1)
summary(g1)

g2<- datos[datos$grupo==2,]
prop.table(table(g2$Species))*100
g3<- datos[datos$grupo==3,]
prop.table(table(g3$Species))*100

plotcluster(iris[,1:4],km$cluster) #grafica la ubicaciÃ³n de los clusters

#Clustering jerÃ¡rquico
hc<-hclust(dist(iris[,1:4])) #Genera el clustering jerÃ¡rquico de los datos
plot(hc) #Genera el dendograma
rect.hclust(hc,k=3) #Dibuja el corte de los grupos en el grÃ¡fico
groups<-cutree(hc,k=3) #corta el dendograma, determinando el grupo de cada fila
datos$gruposHC<-groups


g1HC<-datos[datos$gruposHC==1,]
g2HC<-datos[datos$gruposHC==2,]
g3HC<-datos[datos$gruposHC==3,]

#Fuzzy C-Means
fcm<-cmeans(iris[,1:4],3)
datos$FCGrupos<-fcm$cluster
datos<-cbind(datos,fcm$membership)

#Mixture of gaussians
mc<-Mclust(iris[,1:4],3)
plot(mc, what = "classification", main="MClust Classification")
datos$mxGau<-mc$classification
g1MC<-datos[datos$mxGau==1,]
g2MC<-datos[datos$mxGau==2,]
g3MC<-datos[datos$mxGau==3,]

#MÃ©todo de la silueta para las k-medias
silkm<-silhouette(km$cluster,dist(iris[,1:4]))
mean(silkm[,3]) #0.55, no es la mejor particiÃ³n pero no estÃ¡ mal

#MÃ©todo de la silueta para clustering jerÃ¡rquico
silch<-silhouette(groups,dist(iris[,1:4]))
mean(silch[,3]) #0.51, no es la mejor particiÃ³n pero no estÃ¡ mal

#MÃ©todo de la silueta para fuzzy cmeans
silfcm<-silhouette(fcm$cluster,dist(iris[,1:4]))
mean(silfcm[,3]) #0.54, no es la mejor particiÃ³n pero no estÃ¡ mal

#MÃ©todo de la silueta para mixture of gaussians
silmg<-silhouette(mc$classification,dist(iris[,1:4]))
mean(silmg[,3]) #0.50, no es la mejor particiÃ³n pero no estÃ¡ mal

#MÃ©todo de Ward para determinar el nÃºmero correcto de clusteres con k-medias
#Para saber cual es el mejor numero de clusters
wss <- (nrow(iris[,1:4])-1)*sum(apply(iris[,1:4],2,var))

for (i in 2:10) 
  wss[i] <- sum(kmeans(iris[,1:4], centers=i)$withinss)

plot(1:10, wss, type="b", xlab="Number of Clusters",  ylab="Within groups sum of squares")

#Paquete para saber el mejor número de clusters
nb <- NbClust(iris[,1:4], distance = "euclidean", min.nc = 2,
              max.nc = 10, method = "complete", index ="all")

#Visualización de los clusters con factoextra
#Visualización de las k-medias
fviz_cluster(km, data = iris[,1:4],geom = "point", ellipse.type = "norm")

#Visualización de cluster jerárquico
hc.cut<-hcut(iris[,1:4], k=3, hc_method = "complete")
fviz_dend(hc.cut, show_labels = FALSE, rect = TRUE)
fviz_cluster(hc.cut, ellipse.type = "convex")
